{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PISA of Ising MIRT Ising Theta($\\theta_2$) and Q reSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_spss(\"CY07_MSU_STU_COG_testlet.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the Process was Carried Out With Pandas\n",
    "\n",
    "fil1 = raw_df.iloc[:, 13:65]      # Clear All the Information Except Responses\n",
    "\n",
    "fil2 = fil1.replace(['Full credit', '1 - Full credit', '2 - Full credit', 'No credit', '0 - No credit'], [1, 1, 1, 0, 0])\n",
    "# Invert All Responses in terms of Binary Codes(1: Correct, 0: Incorrect)\n",
    "\n",
    "fil3 = fil2.drop('CM955Q03S', axis=1)    # Clear the Item of Multiple Choices\n",
    "fil4 = fil3.dropna(how='all')            # Clear All Students of No Responses & Clear All Items of No Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to Numpy\n",
    "num_np = fil4.to_numpy()\n",
    "\n",
    "# The Conversion Process to avoid 'divided by zero' error\n",
    "scarub_np = np.where(num_np == 1, 0.99, num_np)\n",
    "scourge_np = np.where(scarub_np == 0, 0.01, scarub_np)\n",
    "num_df = scourge_np                                     # Caution!! num_df is of numpy, not of pandas!!\n",
    "\n",
    "# print(num_df)\n",
    "\n",
    "num_dfdf = pd.DataFrame(num_df)                         # num_dfdf is, at last, of pandas!\n",
    "p_solves = num_dfdf.notnull().sum(1)                    # count the number of responses regardless of NaN\n",
    "\n",
    "# Data shape\n",
    "rows, columns = num_df.shape\n",
    "# print(rows, columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Responses for the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gagongs = []\n",
    "test_gagongs = []\n",
    "num_iter = 10\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    # 1st, input the shared data from UIRT.\n",
    "    train_carrier_df = pd.read_csv(\"share_traindf0.1_{0}_1518_0701.csv\".format(i+1))\n",
    "    test_carrier_df = pd.read_csv(\"share_testdf0.1_1518_{0}_0701.csv\".format(i+1))\n",
    "    \n",
    "    train_carrier_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    test_carrier_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    \n",
    "    train_gagongs.append(train_carrier_df)\n",
    "    test_gagongs.append(test_carrier_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Functions of Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for the comparison with the reference data.\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def expect_model(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let):\n",
    "    \n",
    "    gagong_let = train_gagong_let.to_numpy()\n",
    "    exponet_1 = alpha_let * theta_let\n",
    "    exponet_2 = alpha2le * theta2le\n",
    "    \n",
    "    before_nan = np.exp(exponet_1 + exponet_2 - d_let)/(1+np.exp(exponet_1 + exponet_2 - d_let))\n",
    "    after_nan = before_nan.copy()\n",
    "    \n",
    "    # Reflection of NaN data\n",
    "    for n in range(before_nan.shape[0]):\n",
    "        for m in range(before_nan.shape[1]):\n",
    "            if np.isnan(gagong_let[n][m]):\n",
    "                after_nan[n][m] = np.nan\n",
    "    \n",
    "    # The Conversion Process to avoid 'divided by zero' error\n",
    "    scarub = np.where(after_nan > 0.99, 0.99, after_nan)\n",
    "    scourge = np.where(scarub < 0.01, 0.01, scarub)\n",
    "    model_result = scourge\n",
    "    \n",
    "    return model_result                  # The result is yielded in numpy form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parts of chain rule of D_KL's derivative.\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def preprocess_diff(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let):\n",
    "    \n",
    "    gagong_let = train_gagong_let.to_numpy()\n",
    "    p_imu = expect_model(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let)  # from the model\n",
    "    q_imu = gagong_let.copy()                                                                # from the reference data\n",
    "\n",
    "    # 바로 p와 q 조합\n",
    "    KLD_common = p_imu - q_imu           # KLD represents the initial of 'K'ullback-'L'eibler 'D'ivergence.\n",
    "    \n",
    "    return KLD_common                   # The result is yielded in 2D numpy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to update alpha\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def set_alpha(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let):\n",
    "    \n",
    "    # Loading the common part\n",
    "    common_unit = preprocess_diff(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let)\n",
    "    \n",
    "    # Calculation Start\n",
    "    delta_matrix = theta_let * common_unit                         # before summation (formed in numpy)\n",
    "    \n",
    "    # Get rid of Missing Data\n",
    "    dmatrix_df = pd.DataFrame(delta_matrix)                        \n",
    "    dmatrix_fna = dmatrix_df.fillna(0)                             \n",
    "    delta_matrix2 = dmatrix_fna.to_numpy()\n",
    "    \n",
    "    delta_alphak = delta_matrix2.sum(axis=0, keepdims = True)      # summation in terms of examinees\n",
    "    \n",
    "    alpha_med = alpha_let - A * delta_alphak             # alpha update by means of Gradient Descent\n",
    "    alpha_result = alpha_med\n",
    "    \n",
    "    return alpha_result                                 # The result is yielded in numpy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to update d\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def set_delta(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let):\n",
    "    \n",
    "    # Loading the common part\n",
    "    common_unit = preprocess_diff(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let)\n",
    "    \n",
    "    # Calculation Start\n",
    "    delta_matrix = (-1) * common_unit                         # before summation (formed in numpy)\n",
    "    \n",
    "    # Get rid of Missing Data\n",
    "    dmatrix_df = pd.DataFrame(delta_matrix)                        \n",
    "    dmatrix_fna = dmatrix_df.fillna(0)                             \n",
    "    delta_matrix2 = dmatrix_fna.to_numpy()\n",
    "    \n",
    "    delta_dtak = delta_matrix2.sum(axis=0, keepdims = True)      # summation in terms of examinees\n",
    "    \n",
    "    d_med = d_let - A * delta_dtak                            # d update by means of Gradient Descent\n",
    "    d_result = d_med - np.mean(d_med)                         # standardization of d\n",
    "\n",
    "    return d_result                                          # The result is yielded in numpy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to update theta_1\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def update_theta(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let):\n",
    "    \n",
    "    # Loading the common part\n",
    "    common_unit = preprocess_diff(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let)\n",
    "    \n",
    "    # Calculation Start\n",
    "    delta_matrix = alpha_let * common_unit                         # before summation (formed in numpy)\n",
    "    \n",
    "    # Get rid of Missing Data\n",
    "    dmatrix_df = pd.DataFrame(delta_matrix)                        \n",
    "    dmatrix_fna = dmatrix_df.fillna(0)                             \n",
    "    delta_matrix2 = dmatrix_fna.to_numpy()\n",
    "    \n",
    "    delta_thetak = delta_matrix2.sum(axis=1, keepdims = True)   # summation in terms of items\n",
    "        \n",
    "    theta_update = theta_let - A * delta_thetak                 # theta_1 update by means of Gradient Descent\n",
    "\n",
    "    return theta_update                                         # The result is yielded in numpy form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Functions for Updating $\\theta_{2}$ Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"theta2_transform.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"theta2_update.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both samjin_data and Q_let are of numpy.    'samjin' means 'trinary' in Korean.\n",
    "def Shell_gagong(samjin_data, Q_let):\n",
    "    \n",
    "    num_gagong = samjin_data.copy()\n",
    "    rows_let = num_gagong.shape[0]\n",
    "    columns_let = num_gagong.shape[1]\n",
    "\n",
    "    shell_list = []\n",
    "\n",
    "    for i in range(rows_let):\n",
    "        garo_pre = num_gagong[i, :]                      # response vector(Y) of !D. 'garo' means 'horizon' in Korean.\n",
    "        garo_T = np.reshape(garo_pre, (columns_let, 1))  # vertical form\n",
    "        sero = garo_T.copy()                             # 'sero' means 'the vertical' in Korean.\n",
    "        garo = np.transpose(garo_T)\n",
    "        shell_rough = sero * garo                    # 2D matrix of all the combination of Y_i Y_j (symmetric)\n",
    "\n",
    "        carrier = Q_let * shell_rough                # 2D matrix with intensity Q\n",
    "        np.fill_diagonal(carrier, 0)                 # off-diagonal\n",
    "\n",
    "        shell_list.append(carrier)\n",
    "\n",
    "    shell_result = np.array(shell_list)          # The result is yielded in 3-Rank Tensor\n",
    "    \n",
    "    return shell_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to generate ingredient for pseudo-probability from Ising Hamiltonian\n",
    "# Gagong_data is of pandas and Q_let is of numpy.\n",
    "def answer_covari_bfsum(gagong_data, Q_let):\n",
    "\n",
    "    num_gagong_bf = gagong_data.to_numpy()\n",
    "    rows_let = num_gagong_bf.shape[0]\n",
    "    columns_let = num_gagong_bf.shape[1]\n",
    "    Yij_shell_let = Yij_shell.copy()\n",
    "\n",
    "    Q_np = Q_let.copy()\n",
    "    \n",
    "    # Conversion of (1,0) binary data into (1,-1) binary data (NaN is transformed to zero)\n",
    "    # Refinement for avoiding 'divided by zero' error\n",
    "    num_gagonged_bf = np.where(num_gagong_bf == 0.01, -0.99, num_gagong_bf)\n",
    "    num_gag_pd = pd.DataFrame(num_gagonged_bf)\n",
    "    num_gag_fna = num_gag_pd.fillna(0)\n",
    "    num_gagonged_np = num_gag_fna.to_numpy()\n",
    "    \n",
    "    p_bfsum = Shell_gagong(num_gagonged_np, Q_np)       # the numberator before sum of the formula above\n",
    "\n",
    "#---------------------------------------------------simple sum up ------------- Normalization down ---------------------------------------\n",
    "    \n",
    "    # generation of denominator of the formula above\n",
    "    \n",
    "    denomin = []\n",
    "    for i in range(rows):\n",
    "        bf_Qsam = Yij_shell[i] * Q_let\n",
    "        af_Qsam = bf_Qsam.sum()\n",
    "        denomin.append(af_Qsam)\n",
    "\n",
    "    P2_carrier = p_bfsum.copy()          # 3-Rank Tensor\n",
    "    \n",
    "    # Ingredient of pseudo-probability\n",
    "    for i in range(rows_let):\n",
    "        if denomin[i] == 0:\n",
    "            P2_carrier[i] = 0 * P2_carrier[i] # Get rid of the information of examinees who solved only one item.\n",
    "        else:\n",
    "            P2_carrier[i] = P2_carrier[i] / denomin[i]\n",
    "    \n",
    "    return P2_carrier\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of pseudo-probability is accomplished in the end.\n",
    "# Gagong_data is of pandas and Q_let is of numpy.\n",
    "def answer_covari_afsum(gagong_data, Q_let):\n",
    "    \n",
    "    # collection of the whole ingredient\n",
    "    p_bfsum = answer_covari_bfsum(gagong_data, Q_let)\n",
    "    gagong_np = gagong_data.to_numpy()\n",
    "    rows_let = gagong_np.shape[0]\n",
    "    columns_let = gagong_np.shape[1]\n",
    "    \n",
    "    covari_ini = p_bfsum.sum(axis=2)\n",
    "    covari_mid = covari_ini.sum(axis=1)\n",
    "    covari_carry = np.reshape(covari_mid, (rows_let, 1))  # keep the vertical shape\n",
    "\n",
    "    # pseudo-probability of range between 0 and 1\n",
    "\n",
    "    mid_result = (49/98.01) * (covari_carry) + 0.5  # refinement avoding 'divided by zero' error\n",
    "    \n",
    "    # refinement avoding 'divided by zero' error\n",
    "    scarub = np.where(mid_result > 0.99, 0.99, mid_result)\n",
    "    scourge = np.where(scarub < 0.01, 0.01, scarub)\n",
    "    P2_result = scourge\n",
    "        \n",
    "    return P2_result       # pseudo-probability of numpy form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to calculate the derivative of KLD by Q\n",
    "# Gagong_data is of pandas the others are of numpy.\n",
    "def Q_deriv(alp1, alp2, d_let, tht1, tht2, Q_let, gagong_data):\n",
    "\n",
    "    num_gagong_bf = gagong_data.to_numpy()\n",
    "    rows_let = num_gagong_bf.shape[0]\n",
    "    columns_let = num_gagong_bf.shape[1]\n",
    "    Yij_shell_let = Yij_shell.copy()\n",
    "\n",
    "    Q_np = Q_let.copy()             \n",
    "    Q_nuul = Q_halves.copy()       # The initialized Q matrix of Universality\n",
    "    \n",
    "    # Conversion of (1,0) binary data into (1,-1) binary data (NaN is transformed to zero)\n",
    "    # Refinement for avoiding 'divided by zero' error\n",
    "    num_gagonged_bf = np.where(num_gagong_bf == 0.01, -0.99, num_gagong_bf)\n",
    "    num_gag_pd = pd.DataFrame(num_gagonged_bf)\n",
    "    num_gag_fna = num_gag_pd.fillna(0)\n",
    "    num_gagonged_np = num_gag_fna.to_numpy()\n",
    "    \n",
    "    p_bfsum_nossi = Shell_gagong(num_gagonged_np, Q_nuul)\n",
    "    p_bfsum = Shell_gagong(num_gagonged_np, Q_np)         # Before calculation\n",
    "\n",
    "#---------------------------------------------------division line-------------------------------#\n",
    "    \n",
    "    # generation of denominator of the formula above\n",
    "    denomin = []\n",
    "    for i in range(rows):\n",
    "        bf_Qsam = Yij_shell[i] * Q_let\n",
    "        af_Qsam = bf_Qsam.sum()\n",
    "        denomin.append(af_Qsam)\n",
    "\n",
    "    P2_carrier1 = p_bfsum_nossi.copy()\n",
    "    P2_carrier20 = p_bfsum.copy()          # 3-Rank Tensor\n",
    "\n",
    "    # The 1st term of the numerator\n",
    "    for i in range(rows_let):\n",
    "        if denomin[i] == 0:\n",
    "            P2_carrier1[i] = 0 * P2_carrier1[i]\n",
    "        else:\n",
    "            P2_carrier1[i] = P2_carrier1[i] / denomin[i]\n",
    "\n",
    "    # The 2nd term of the numerator\n",
    "    for i in range(rows_let):\n",
    "        if denomin[i] == 0:\n",
    "            P2_carrier20[i] = 0 * P2_carrier20[i]\n",
    "        else:\n",
    "            P2_carrier20[i] = P2_carrier20[i] / (denomin[i] * denomin[i])\n",
    "            \n",
    "    covari2_ini = P2_carrier20.sum(axis=2)\n",
    "    covari2_mid = covari2_ini.sum(axis=1)\n",
    "    P22_part = np.reshape(covari2_mid, (rows_let, 1))      # keep the vertical shape\n",
    "    \n",
    "    P2_list = []\n",
    "    for i in range(rows_let):\n",
    "        carrier = Yij_shell_let[i] * P22_part[i]\n",
    "        P2_list.append(carrier)\n",
    "    \n",
    "    P2_carrier2 = np.array(P2_list)\n",
    "    \n",
    "    return P2_carrier1, P2_carrier2 # former: the 1st term, latter: the 2nd term of the numerator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to sum all the ingredient of the formula above in the end\n",
    "# Gagong_data is of pandas the others are of numpy.\n",
    "def Q_learn(alp1, alp2, d_let, tht1, tht2, Q_let, gagong_data):\n",
    "    \n",
    "    Q_np_test = Q_let.copy()               # Matrix to be learned\n",
    "    gagonged_data = gagong_data.to_numpy()\n",
    "    rows_let = gagonged_data.shape[0]\n",
    "    columns_let = gagonged_data.shape[1]\n",
    "\n",
    "    # the chain of the derivative: 3-Rank Tensor form\n",
    "    P2_mu = answer_covari_afsum(gagong_data, Q_np_test)\n",
    "    Normed_Y = (49/98.01) * (Q_deriv(alp1, alp2, d_let, tht1, tht2, Q_let, gagong_data)[0] - Q_deriv(alp1, alp2, d_let, tht1, tht2, Q_let, gagong_data)[1])\n",
    "\n",
    "#---------------------------------------------------division line----------------------------------------------------------------------------------#\n",
    "    \n",
    "    # common part\n",
    "    com_pt = preprocess_diff(alp1, alp2, d_let, tht1, tht2, gagong_data)\n",
    "\n",
    "    # calculation start\n",
    "    common_unit_np = com_pt * alp1                               # 2-dimensional Matrix\n",
    "        \n",
    "    common_unit_T = np.transpose(common_unit_np)                 # mu for axis=1; in order to link mu with 3-Rank Tensor\n",
    "    decoy_1st = pd.DataFrame(common_unit_T)\n",
    "    decoy_2nd = decoy_1st.fillna(0)\n",
    "    common_unit = decoy_2nd.to_numpy()                           \n",
    "\n",
    "#-----------------------------------Now, it's time to build a 4-Rank tensor ------------------------------------------------------------------------#\n",
    "    \n",
    "    P_hat_list = []                              # Initialize the list to store a 4-Rank Tensor\n",
    "    P_hat_3D = []                                # Initialize the list to store a 3-Rank Tensor\n",
    "    carrier_2D = []\n",
    "\n",
    "    for i in range(columns_let):\n",
    "        for j in range(columns_let):\n",
    "            for mu in range(rows_let):\n",
    "                carrier = common_unit[:, mu] * Normed_Y[mu, i, j] / (P2_mu[mu, 0] * (1 - P2_mu[mu, 0]))\n",
    "                carrier_2D.append(carrier)\n",
    "            P_hat_3D.append(carrier_2D)          # combination of mu and k components is added.\n",
    "            carrier_2D = []                      # Reset the 2D matrix\n",
    "        P_hat_list.append(P_hat_3D)              # complete the ith component\n",
    "        P_hat_3D = []                            # Reset the 3-Rank Tensor\n",
    "        \n",
    "    P_hat_np = np.array(P_hat_list)              # complete the 4-Rank Tensor\n",
    "    \n",
    "    #Then, sum it up in terms of k and mu axes.\n",
    "    # KLD Gradient Discent\n",
    "    Q_pre = P_hat_np.sum(axis=3)                 # sum it up in mu axis\n",
    "    Q_presum = Q_pre.sum(axis=2)                 # sum it up in k axis\n",
    "    \n",
    "    # Final Gradient Descendent: update\n",
    "    Q_med = Q_np_test - A * Q_presum\n",
    "    np.fill_diagonal(Q_med, 0)\n",
    "    Q_result = Q_med/(2 * Q_med.mean())          # Normalization: the average of all the component should be 0.5.\n",
    "\n",
    "    return Q_result                             # The result is yielded in 2D matrix of numpy form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to update theta_2\n",
    "# Only the gagong_data is given in pandas.\n",
    "# Theta_2 is updated via the imaged process above.\n",
    "def set_theta_Q(gagong_data, Q_let):\n",
    "    \n",
    "    rate_result = answer_covari_afsum(gagong_data, Q_let)\n",
    "    \n",
    "    theta_result = np.log((rate_result)/(1 - rate_result))\n",
    "    \n",
    "    return theta_result       # The result is yielded in numpy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to calculate KLD\n",
    "# Train_gagong_let is of pandas the others are of numpy.\n",
    "def set_D_KL(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let):\n",
    "    \n",
    "    num_gagong = train_gagong_let.to_numpy()\n",
    "    P_imu = expect_model(alpha_let, alpha2le, d_let, theta_let, theta2le, train_gagong_let)\n",
    "    Q_imu = num_gagong.copy()\n",
    "    \n",
    "    KLD_imu_np = Q_imu * np.log((Q_imu) / (P_imu)) + (1 - Q_imu) * np.log((1 - Q_imu)/(1 - P_imu))\n",
    "    \n",
    "    # Get rid of Missing Data\n",
    "    KLD_imu_df = pd.DataFrame(KLD_imu_np)\n",
    "    KLD_shuttle = KLD_imu_df.fillna(0)\n",
    "    KLD_imu = KLD_shuttle.to_numpy()\n",
    "    \n",
    "    D_KL_mu = KLD_imu.sum(axis=1)\n",
    "    D_KL = D_KL_mu.sum(axis=0)\n",
    "    \n",
    "    return D_KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration function for model optimization\n",
    "\n",
    "def opt_model(alpha_let, d_let, theta_let, theta2le, Q_let, train_gagong_let, test_gagong_let, num_iter):\n",
    "\n",
    "    # Initialization of parameters and variables\n",
    "    alpha1_test = alpha_let.copy()\n",
    "    alpha2_test = alpha_let.copy()\n",
    "    d_test = d_let.copy()\n",
    "    Q_test = Q_let.copy()\n",
    "    theta1_test = theta_let.copy()\n",
    "    theta2_test = theta2le.copy()\n",
    "\n",
    "    KLD_train = set_D_KL(alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, train_gagong_let)\n",
    "    KLD_Trains = []\n",
    "    KLD_Trains.append(KLD_train)\n",
    "    \n",
    "    KLD_testset = set_D_KL(alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, test_gagong_let)\n",
    "    KLD_Tests = []\n",
    "    KLD_Tests.append(KLD_testset)\n",
    "    \n",
    "    for k in tqdm(range(num_iter)):\n",
    "\n",
    "        # alpha1 update\n",
    "        alpha1_carrier = set_alpha(alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, train_gagong_let)\n",
    "        alpha1_test = alpha1_carrier\n",
    "\n",
    "        # alpha2 update\n",
    "        alpha2_carrier = set_alpha(alpha2_test, alpha1_test, d_test, theta2_test, theta1_test, train_gagong_let)\n",
    "        alpha2_test = alpha2_carrier\n",
    "\n",
    "        # d update\n",
    "        delta_carrier = set_delta(alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, train_gagong_let)\n",
    "        d_test = delta_carrier\n",
    "\n",
    "        # Q update\n",
    "        Q_carrier = Q_learn(alpha2_test, alpha1_test, d_test, theta2_test, theta1_test, Q_test, train_gagong_let)\n",
    "        Q_test = Q_carrier\n",
    "\n",
    "        # theta1 update\n",
    "        theta1_carrier = update_theta(alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, train_gagong_let)\n",
    "        theta1_test = theta1_carrier\n",
    "\n",
    "        # theta2 update\n",
    "        theta2_carrier = set_theta_Q(train_gagong_let, Q_test)\n",
    "        theta2_test = theta2_carrier\n",
    "\n",
    "        # calculation of Kullback-Leibler Divergence\n",
    "        KLD_carrier = set_D_KL(alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, train_gagong_let)\n",
    "        KLD_testset = set_D_KL(alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, test_gagong_let)\n",
    "        #print(\"Kullback-Leibler Divergence of the Iteration %d: \" % (k+1), KLD_carrier)\n",
    "\n",
    "        # Determination whether the iteration keeps or not\n",
    "        if (k < num_iter - 1) and (KLD_carrier < KLD_train):\n",
    "            KLD_train = KLD_carrier\n",
    "            KLD_Trains.append(KLD_train)        # store KLD of trian set\n",
    "            KLD_Tests.append(KLD_testset)       # store KLD of test set\n",
    "        elif k == num_iter - 1:                # the final iteration\n",
    "            KLD_train = KLD_carrier\n",
    "            KLD_Trains.append(KLD_train)        # store of KLD of train set\n",
    "            KLD_Tests.append(KLD_testset)       # store of KLD of test set\n",
    "            print(\"Final Kullback-Leibler Divergence: \", KLD_train)\n",
    "\n",
    "        else:\n",
    "            print(\"Final Kullback-Leibler Divergence: \", KLD_train)\n",
    "\n",
    "            break\n",
    "            \n",
    "    return alpha1_test, alpha2_test, d_test, theta1_test, theta2_test, Q_test, KLD_Trains, KLD_Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, it is very time to play the real game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Body of the Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "albetheQKLD = []\n",
    "num_iter = 0\n",
    "#train_trial = []\n",
    "#train_trial.append(train_gagongs[0])\n",
    "\n",
    "#for gagong_carrier in train_trial:\n",
    "for gagong_carrier in train_gagongs:\n",
    "    carrier_shell = []\n",
    "\n",
    "    num_dfdf = gagong_carrier.copy()\n",
    "    p_df = num_dfdf.copy()\n",
    "    num_np = num_dfdf.to_numpy()\n",
    "\n",
    "    # theta_1 initialization\n",
    "    row_pre = p_df.mean(axis=1)\n",
    "    row_prob_1 = row_pre.to_numpy()\n",
    "    row_prob = np.reshape(row_prob_1, (rows,1))\n",
    "\n",
    "    theta_1 = np.log(row_prob/(1-row_prob))\n",
    "\n",
    "    # d initialization\n",
    "    col_pre = p_df.mean(axis=0)\n",
    "    col_prob_1 = col_pre.to_numpy()\n",
    "    col_prob = np.array([col_prob_1])\n",
    "    d0 = np.log(col_prob/(1-col_prob))\n",
    "    d = np.mean(d0) - d0\n",
    "\n",
    "    # alpha_1 and alpha_2 initialization\n",
    "    alpha = np.ones((1,columns))\n",
    "    \n",
    "    A = 0.005            # learning rate\n",
    "\n",
    "    # transformation of (1,0) binary responses into (1,-1) binary responses\n",
    "    num_exp1 = num_np.copy()\n",
    "    num_exp2 = np.where(num_exp1 == 0.01, -0.99, num_exp1) # transformation\n",
    "\n",
    "    num_exp_df = pd.DataFrame(num_exp2)\n",
    "    num_exp_af = num_exp_df.fillna(0)                 # get rid of NaN\n",
    "    num_exp_np = num_exp_af.to_numpy()\n",
    "\n",
    "    # Q initialization\n",
    "    Q_np_ini = np.ones((columns, columns))\n",
    "    np.fill_diagonal(Q_np_ini, 0)\n",
    "    Q_halves = Q_np_ini / 2\n",
    "\n",
    "    # theta_2 initialization\n",
    "    shell_list = []\n",
    "\n",
    "    for i in range(rows):\n",
    "        garo_pre = num_exp_np[i, :]\n",
    "        garo_T = np.reshape(garo_pre, (columns, 1))   # vertial vector form\n",
    "        sero = garo_T.copy()\n",
    "        garo = np.transpose(garo_T)\n",
    "        carrier = sero * garo                   \n",
    "        np.fill_diagonal(carrier, 0)                 # off-diagonal\n",
    "\n",
    "        shell_list.append(carrier)\n",
    "\n",
    "    shell_ini = np.array(shell_list)          # initial combination of Y_iY_j\n",
    "\n",
    "    # the reference to indicate the location of solved items\n",
    "    Y_solved0 = num_np.copy()\n",
    "    Y_solved1 = np.where(Y_solved0 == 0.01, 1, Y_solved0)\n",
    "    Y_solved2 = np.where(Y_solved1 == 0.99, 1, Y_solved1)\n",
    "    Y_pd = pd.DataFrame(Y_solved2)\n",
    "    Y_fna = Y_pd.fillna(0)         # set NaN as zero\n",
    "    Y_solved = Y_fna.to_numpy()\n",
    "\n",
    "    Yij_solved = []\n",
    "\n",
    "    for i in range(rows):\n",
    "        garo_pre = Y_solved[i, :]\n",
    "        garo_T = np.reshape(garo_pre, (columns, 1))\n",
    "        sero = garo_T.copy()\n",
    "        garo = np.transpose(garo_T)\n",
    "        carrier = sero * garo                   \n",
    "        np.fill_diagonal(carrier, 0)            \n",
    "\n",
    "        Yij_solved.append(carrier)\n",
    "\n",
    "    Yij_shell = np.array(Yij_solved)\n",
    "    \n",
    "    denominator = []\n",
    "    for i in range(rows):\n",
    "        bf_Qsum = Yij_shell[i] * Q_halves\n",
    "        af_Qsum = bf_Qsum.sum()\n",
    "        denominator.append(af_Qsum)        # generation of the denominator\n",
    "\n",
    "    P_carrier = []     # basket for initial pseudo-probability\n",
    "    for i in range(rows):\n",
    "        garo_pre = num_exp_np[i, :]\n",
    "        garo = np.reshape(garo_pre, (1, columns))\n",
    "        sero_T = np.copy(garo)\n",
    "        sero = np.transpose(sero_T)\n",
    "\n",
    "        vectorman1 = sero * garo\n",
    "        vectorman11 = Q_halves * vectorman1\n",
    "        vectorman111 = vectorman11.sum(axis=1)\n",
    "        vectorman2 = vectorman111.sum(axis=0)\n",
    "\n",
    "        if denominator[i] == 0:\n",
    "            P_mu = 0\n",
    "        else:\n",
    "            P_mu = vectorman2 / denominator[i]\n",
    "\n",
    "        P_carrier.append(P_mu)\n",
    "\n",
    "    P_norm = np.array(P_carrier)\n",
    "\n",
    "    theta_pre = (49/98.01) * (P_norm) + 0.5    # final form of pseudo-probability initialization\n",
    "\n",
    "    # final initialization of theta_2\n",
    "    theta1_bfT = np.log(theta_pre / (1 - theta_pre))\n",
    "    theta_2 = np.reshape(theta1_bfT, (rows,1))\n",
    "\n",
    "    # initialization of the probability distribution of the model\n",
    "    exp1 = alpha * theta_1\n",
    "    exp2 = alpha * theta_2\n",
    "    ex_prob = np.exp(exp1 + exp2 - d)/(1+np.exp(exp1 + exp2 - d))\n",
    "\n",
    "    ex_prob_real = ex_prob.copy()\n",
    "\n",
    "    for n in range(ex_prob.shape[0]):        # reflect the distribution of NaN\n",
    "        for m in range(ex_prob.shape[1]):\n",
    "            if np.isnan(num_np[n][m]):\n",
    "                ex_prob_real[n][m] = np.nan\n",
    "\n",
    "    # KLD of each response\n",
    "    KLD_indiv = num_np * np.log(num_np / ex_prob_real) + (1 - num_np) * np.log((1 - num_np) / (1 - ex_prob_real))\n",
    "\n",
    "    # get rid of missing values\n",
    "    KLD_indiv_df = pd.DataFrame(KLD_indiv)\n",
    "    KLD_NaNga_df = KLD_indiv_df.fillna(0)\n",
    "    KLD_NaNga_np = KLD_NaNga_df.to_numpy()\n",
    "\n",
    "    # KLD initialization\n",
    "    KLD_RowSum = np.sum(KLD_NaNga_np, axis=1)\n",
    "    KLD_TotalSum_np = np.sum(KLD_RowSum, axis=0)\n",
    "\n",
    "    # Model Optimization Start\n",
    "    alpha1_mod, alpha2_mod, d_mod, theta1_mod, theta2_mod, Q_mod, KLDs_mod, KLDs_test_mod = opt_model(alpha, d, theta_1, theta_2, Q_halves, p_df, test_gagongs[num_iter], 20)\n",
    "\n",
    "    # save for further analysis\n",
    "    carrier_shell.append(alpha1_mod)       # 0\n",
    "    carrier_shell.append(alpha2_mod)       # 1\n",
    "    carrier_shell.append(d_mod)            # 2\n",
    "    carrier_shell.append(theta1_mod)       # 3\n",
    "    carrier_shell.append(theta2_mod)       # 4\n",
    "    carrier_shell.append(Q_mod)            # 5\n",
    "    carrier_shell.append(KLDs_mod)         # 6\n",
    "    carrier_shell.append(KLDs_test_mod)    # 7\n",
    "\n",
    "    albetheQKLD.append(carrier_shell)\n",
    "    num_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(albetheQKLD)\n",
    "print(albetheQKLD[0][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_save = 0\n",
    "for carrying in albetheQKLD:\n",
    "    num_save+=1\n",
    "    \n",
    "    KLD_trained = carrying[6]\n",
    "    KLD_tested = carrying[7]\n",
    "    \n",
    "    KLDs_train_pd = pd.DataFrame(KLD_trained)\n",
    "    KLDs_test_pd = pd.DataFrame(KLD_tested)\n",
    "\n",
    "    KLDs_train_pd.to_csv(\"MIRT{0}_KLDs_50_0.1train_0705.csv\".format(num_save))\n",
    "    KLDs_test_pd.to_csv(\"MIRT{0}_KLDs_50_0.1test_0705.csv\".format(num_save))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set vs Train set and Save the Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expect_simple_cal(alpha1_let, alpha2_let, d_let, theta1_let, theta2_let):\n",
    "    exp1 = alpha1_let * theta1_let\n",
    "    exp2 = alpha2_let * theta2_let\n",
    "\n",
    "    cal2 = np.exp(exp1 + exp2 - d_let)/(1+np.exp(exp1 + exp2 - d_let))\n",
    "    \n",
    "    if cal2 >= 0.99:\n",
    "        cal2 = 0.99\n",
    "    elif cal2 <= 0.01:\n",
    "        cal2 = 0.01\n",
    "\n",
    "    cal_result = cal2 \n",
    "\n",
    "    return cal_result   # return in number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "basket_final = []\n",
    "num_unit = 0\n",
    "\n",
    "for gagong_unit in test_gagongs:\n",
    "    # pick up a basket of parameters\n",
    "    basket_picks = albetheQKLD.copy()[num_unit]\n",
    "\n",
    "    # update the index\n",
    "    num_unit += 1\n",
    "\n",
    "    # distribution of numbers for thetas\n",
    "    theta1_fin_df = pd.DataFrame(basket_picks[3])\n",
    "    theta2_fin_df = pd.DataFrame(basket_picks[4])\n",
    "\n",
    "    # namtaging of alphas and d\n",
    "    alpha1_fin_df = pd.DataFrame(basket_picks[0])\n",
    "    alpha2_fin_df = pd.DataFrame(basket_picks[1])\n",
    "    d_fin_df = pd.DataFrame(basket_picks[2])\n",
    "\n",
    "    alpha1_fin_df.columns = fil4.columns.to_list()\n",
    "    alpha2_fin_df.columns = fil4.columns.to_list()\n",
    "    d_fin_df.columns = fil4.columns.to_list()\n",
    "\n",
    "    # store the all the information about test set\n",
    "    threshed_theta = []\n",
    "\n",
    "    # Set the coordinate\n",
    "    coord_pd = pd.read_csv(\"UIRT_0.1testset_{0}_0701.csv\".format(num_unit))\n",
    "    coord_pd.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    sampl_len = coord_pd.shape[1]\n",
    "    \n",
    "    for th in range(sampl_len):\n",
    "        piece_th = []\n",
    "        \n",
    "        coord_x = int(coord_pd.loc[0][th])\n",
    "        coord_col = coord_pd.loc[1][th]\n",
    "        correctness = float(coord_pd.loc[2][th])\n",
    "\n",
    "        theta1_piece = theta1_fin_df[0][coord_x]\n",
    "        alpha1_piece = alpha1_fin_df.loc[0][coord_col]\n",
    "        theta2_piece = theta2_fin_df[0][coord_x]\n",
    "        alpha2_piece = alpha2_fin_df.loc[0][coord_col]\n",
    "        d_piece = d_fin_df.loc[0][coord_col]\n",
    "        expect_cal = expect_simple_cal(alpha1_piece, alpha2_piece, \n",
    "                                       d_piece, theta1_piece, theta2_piece)\n",
    "\n",
    "        piece_th.append(coord_x)         # 0\n",
    "        piece_th.append(coord_col)       # 1\n",
    "        piece_th.append(correctness)     # 2\n",
    "        piece_th.append(theta1_piece)    # 3\n",
    "        piece_th.append(alpha1_piece)    # 4\n",
    "        piece_th.append(theta2_piece)    # 5\n",
    "        piece_th.append(alpha2_piece)    # 6\n",
    "        piece_th.append(d_piece)         # 7\n",
    "        piece_th.append(expect_cal)      # 8\n",
    "        threshed_theta.append(piece_th)  # th\n",
    "\n",
    "    #print(threshed_theta)       # 순서: 학생 index, 문제 index, 문제 정오, \n",
    "                                        # theta, alpha, beta, 모델계산값\n",
    "    \n",
    "    threshed_pick = threshed_theta.copy()\n",
    "\n",
    "    # Judgment whether the imputations correct or not\n",
    "    stud_info_simple = []\n",
    "    num_R = 0            # number of right imputation\n",
    "    num_W = 0            # number of wrong imputation\n",
    "    num_tot = 0          # number of total items in the test set\n",
    "\n",
    "    for student in threshed_pick:\n",
    "        carrier_bot = []\n",
    "        data_real = student[2]\n",
    "        data_cal = student[8]\n",
    "        data_jud = 0\n",
    "        data_RW = ''                       # Initialization\n",
    "\n",
    "#        if data_cal >=0.7:                 # rounding off to the nearest integer\n",
    "#            data_jud = 0.99\n",
    "        if data_cal >=0.5:                 # rounding off to the nearest integer\n",
    "            data_jud = 0.99\n",
    "#        elif data_cal <= 0.3:\n",
    "#            data_jud = 0.01\n",
    "#        else:\n",
    "#            data_jud = 0.5\n",
    "        else:\n",
    "            data_jud = 0.01\n",
    "\n",
    "        if data_real == data_jud:\n",
    "            data_RW = 'O'                   # 'O' represents the model imputed correctly.\n",
    "            num_R += 1\n",
    "            num_tot += 1\n",
    "        else:\n",
    "            data_RW = 'X'                   # 'X' represents the model imputed incorrectly.\n",
    "            num_W += 1\n",
    "            num_tot +=1\n",
    "\n",
    "        carrier_bot.append(student[0])      # examinee number\n",
    "        carrier_bot.append(student[1])      # item number\n",
    "        carrier_bot.append(data_RW)         # judgment\n",
    "        stud_info_simple.append(carrier_bot)\n",
    "\n",
    "    print(\"Accordance Ratio of Ising MIRT: {0}\".format(num_R/num_tot * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
