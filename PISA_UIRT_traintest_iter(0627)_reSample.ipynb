{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PISA of UIRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_spss(\"CY07_MSU_STU_COG_testlet.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the Process was Carried Out With Pandas\n",
    "\n",
    "fil1 = raw_df.iloc[:, 13:65]      # Clear All the Information Except Responses\n",
    "\n",
    "fil2 = fil1.replace(['Full credit', '1 - Full credit', '2 - Full credit', 'No credit', '0 - No credit'], [1, 1, 1, 0, 0])\n",
    "# Invert All Responses in terms of Binary Codes(1: Correct, 0: Incorrect)\n",
    "\n",
    "fil3 = fil2.drop('CM955Q03S', axis=1)    # Clear the Item of Multiple Choices\n",
    "fil4 = fil3.dropna(how='all')            # Clear All Students of No Responses & Clear All Items of No Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to Numpy\n",
    "num_np = fil4.to_numpy()\n",
    "\n",
    "# The Conversion Process to avoid 'divided by zero' error\n",
    "scarub_np = np.where(num_np == 1, 0.99, num_np)\n",
    "scourge_np = np.where(scarub_np == 0, 0.01, scarub_np)\n",
    "num_df = scourge_np                                     # Caution!! num_df is of numpy, not of pandas!!\n",
    "\n",
    "# print(num_df)\n",
    "\n",
    "num_dfdf = pd.DataFrame(num_df)                         # num_dfdf is, at last, of pandas!\n",
    "p_solves = num_dfdf.notnull().sum(1)                    # count the number of responses regardless of NaN\n",
    "\n",
    "# Data shape\n",
    "rows, columns = num_df.shape\n",
    "# print(rows, columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Responses for the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_random(num_residues, num_division):       # Number Distribution in Random\n",
    "    \n",
    "    result = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(num_division):\n",
    "        if count < num_residues:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "        count += 1\n",
    "        \n",
    "    random.shuffle(result)\n",
    "    result_np = np.array(result)\n",
    "        \n",
    "    return result_np        # return is yielded in numpy form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colrow_extractor(df_bf_gagong, df_pray_gagong, rate_sam):      # df_pray_gagong is of pandas, list_cols is of list.\n",
    "    \n",
    "    cols_num_samp = []              # the number of samples for each item\n",
    "    coord_list = []\n",
    "    ind_n = 0\n",
    "    \n",
    "    df_decay_train = df_bf_gagong.drop(['NS'], axis=1)\n",
    "    df_decay = df_pray_gagong.drop(['NS'], axis=1)\n",
    "    list_cols = basket_column.copy()\n",
    "    \n",
    "    row_min = df_decay.shape[0]\n",
    "    col_min = df_decay.shape[1]\n",
    "    \n",
    "    num_sam = math.trunc(tot_num_ref * rate_sam)     # tot_num_ref is universal variable.\n",
    "    \n",
    "    # To distribute samples for each item\n",
    "    how_quotient = num_sam // col_min\n",
    "    how_residue = num_sam % col_min\n",
    "    \n",
    "    num_dist_col = simple_random(how_residue, col_min) + how_quotient\n",
    "    num_dist_rsh = num_dist_col.reshape(1,col_min)\n",
    "    num_dist_col_pd = pd.DataFrame(num_dist_rsh)\n",
    "    num_dist_col_pd.columns = list_cols[:51]\n",
    "    \n",
    "    # To distribute samples for each examinee\n",
    "    how_quotient_mu = num_sam // row_min\n",
    "    how_residue_mu = num_sam % row_min\n",
    "    \n",
    "    num_dist_row = simple_random(how_residue_mu, row_min) + how_quotient_mu\n",
    "    num_dist_rshr = num_dist_row.reshape(row_min,1)\n",
    "    num_dist_row_pd = pd.DataFrame(num_dist_rshr, index=df_decay.index.tolist())\n",
    "    \n",
    "    # data for test set\n",
    "    data_collect = []\n",
    "    coord_col = []\n",
    "    coord_row = []\n",
    "    row_col_val = []\n",
    "    \n",
    "    # result for test set\n",
    "    basket_trial_np = np.zeros((rows,columns))\n",
    "    basket_trial_nan = np.where(basket_trial_np == np.nan, basket_trial_np, np.nan)\n",
    "    #print(basket_trial_nan.shape)\n",
    "    basket_test = pd.DataFrame(basket_trial_nan)\n",
    "    basket_test.columns = list_cols[:51]\n",
    "    #print(basket_test)\n",
    "    \n",
    "    # shuffle examinee's index\n",
    "    shf_index = df_decay.index.tolist().copy()\n",
    "    random.shuffle(shf_index)\n",
    "    \n",
    "    for mu in shf_index:\n",
    "\n",
    "        col_decay = list_cols[:51].copy()\n",
    "        \n",
    "        for j in list_cols[:51]:\n",
    "            if np.isnan(df_decay.loc[mu][j]):\n",
    "                col_decay.remove(j)\n",
    "            elif num_dist_col_pd.loc[0][j] == 0:\n",
    "                col_decay.remove(j)\n",
    "        \n",
    "        col_decay_len = len(col_decay)\n",
    "        num_col_pick = num_dist_row_pd.loc[mu][0]\n",
    "        picked = simple_random(num_col_pick, col_decay_len)\n",
    "        picked_np = np.array(picked)\n",
    "        loc_picked = np.where(picked_np == 1)[0]\n",
    "        \n",
    "        for nm in loc_picked:\n",
    "            col_picked = col_decay[nm]\n",
    "            coord_col.append(col_picked)\n",
    "            coord_row.append(mu)\n",
    "            row_col_val.append(df_decay.loc[mu][col_picked])\n",
    "            num_dist_col_pd.loc[0][col_picked] -= 1\n",
    "            df_decay_train.loc[mu][col_picked] = np.nan\n",
    "            \n",
    "            basket_test.loc[mu][col_picked] = df_decay.loc[mu][col_picked]\n",
    "            \n",
    "            \n",
    "    data_collect.append(coord_row)\n",
    "    data_collect.append(coord_col)\n",
    "    data_collect.append(row_col_val)\n",
    "    data_collect_np = np.array(data_collect)\n",
    "    \n",
    "    return df_decay_train, basket_test, data_collect_np      # processed train set, test set and the set of coordinates of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sampling responses to test set\n",
    "\n",
    "basket_ini = pd.concat([num_dfdf, p_solves], axis=1)   # nametagging of num_dfdf\n",
    "\n",
    "num_dfdf_stunt = num_dfdf.copy()                       # num_dfdf's understudent\n",
    "num_dfdf_stunt.columns = fil4.columns.to_list()\n",
    "\n",
    "basket_column = fil4.columns.to_list()\n",
    "basket_column.append('NS')                     # NS stands for 'N'umber of the 'S'olved problems\n",
    "\n",
    "basket_ini.columns = basket_column\n",
    "\n",
    "gagong_univ1 = basket_ini.copy()\n",
    "#gagong_univ21 = gagong_univ1[gagong_univ1['NS'] >= 3]\n",
    "#gagong_univ31 = gagong_univ21.notnull().sum()\n",
    "\n",
    "less_2 = []\n",
    "\n",
    "for i in range(rows):\n",
    "    if basket_ini['NS'][i] <= 15:\n",
    "        less_2.append(i)\n",
    "\n",
    "print(less_2)\n",
    "basket_sel = basket_ini.copy()\n",
    "basket_sel.drop(less_2, axis=0, inplace=True)\n",
    "\n",
    "tot_num_ref = int(gagong_univ1.sum()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gagongs = []\n",
    "test_gagongs = []\n",
    "#num_iter = 1           # At first, beta test\n",
    "num_iter = 10\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    num_df_gagong, test_set_gagong, test_set_coord = random_colrow_extractor(basket_ini, basket_sel, 0.1)\n",
    "    # 'Gumeong' mean 'a hole' in Korean.\n",
    "        \n",
    "    train_gagongs.append(num_df_gagong)\n",
    "    test_gagongs.append(test_set_gagong)\n",
    "    test_coord_pd = pd.DataFrame(test_set_coord)\n",
    "    test_coord_pd.to_csv(\"UIRT_0.1testset_{0}_0701.csv\".format(i+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sav1 = 0\n",
    "\n",
    "for i in range(num_iter):\n",
    "    train_vect = train_gagongs[i].copy()\n",
    "    test_vect = test_gagongs[i].copy()\n",
    "    \n",
    "    train_vect_pd = pd.DataFrame(train_vect)\n",
    "    test_vect_pd = pd.DataFrame(test_vect)\n",
    "    \n",
    "    train_vect_pd.to_csv(\"share_traindf0.1_{0}_1518_0701.csv\".format(i+1))\n",
    "    test_vect_pd.to_csv(\"share_testdf0.1_1518_{0}_0701.csv\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_gagongs[3].notnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Functions of Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for the comparison with the reference data.\n",
    "# All the parameters are given in numpy form.\n",
    "def expect_model(alpha_let, beta_let, theta_let, train_gagong_let):\n",
    "    \n",
    "    gagong_let = train_gagong_let.to_numpy()\n",
    "    exponet_neg = alpha_let * (beta_let - theta_let)\n",
    "    before_nan = 1/ (1 + np.exp(exponet_neg))\n",
    "    after_nan = before_nan.copy()\n",
    "    \n",
    "    # Reflection of NaN data\n",
    "    for n in range(before_nan.shape[0]):\n",
    "        for m in range(before_nan.shape[1]):\n",
    "            if np.isnan(gagong_let[n][m]):\n",
    "                after_nan[n][m] = np.nan\n",
    "                \n",
    "    # The Conversion Process to avoid 'divided by zero' error\n",
    "    scarub_e = np.where(after_nan >= 1, 0.99, after_nan)\n",
    "    scourge_e = np.where(scarub_e <= 0, 0.01, scarub_e)\n",
    "    result_e = scourge_e\n",
    "    \n",
    "    return result_e                  # The result is yielded in numpy form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parts of chain rule of D_KL's derivative.\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def preprocess_diff(alpha_let, beta_let, theta_let, train_gagong_let):\n",
    "\n",
    "    gagong_let = train_gagong_let.to_numpy()\n",
    "    p_imu = expect_model(alpha_let, beta_let, theta_let, train_gagong_let)    # from the model\n",
    "    q_imu = gagong_let.copy()                                   # from the reference data\n",
    "\n",
    "    # 바로 p와 q 조합\n",
    "    KLD_common = p_imu - q_imu\n",
    "    \n",
    "    return KLD_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to update alpha\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def set_alpha(alpha_let, beta_let, theta_let, train_gagong_let):\n",
    "    \n",
    "    # Loading the common part\n",
    "    expo = theta_let - beta_let                                         # exponential term\n",
    "    common_unit = preprocess_diff(alpha_let, beta_let, theta_let, train_gagong_let)\n",
    "    \n",
    "    # Calculation Start\n",
    "    delta_matrix = expo * common_unit                                   # before summation (formed in numpy)\n",
    "    \n",
    "    # Get rid of Missing Data\n",
    "    dmatrix_df = pd.DataFrame(delta_matrix)\n",
    "    dmatrix_fna = dmatrix_df.fillna(0)\n",
    "    delta_matrix2 = dmatrix_fna.to_numpy()\n",
    "    \n",
    "    delta_alphak = delta_matrix2.sum(axis=0, keepdims = True)          # summation in terms of examinees\n",
    "    \n",
    "    alpha_med = alpha_let - A * delta_alphak                    # alpha update by means of Gradient Descent\n",
    "    alpha_result = alpha_med\n",
    "    \n",
    "    return alpha_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to update beta\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def set_beta(alpha_let, beta_let, theta_let, train_gagong_let):\n",
    "    \n",
    "    # Loading the common part\n",
    "    common_unit = preprocess_diff(alpha_let, beta_let, theta_let, train_gagong_let)    \n",
    "    \n",
    "    # Calculation Start\n",
    "    delta_matrix = (-1) * alpha_let * common_unit            # before summation (formed in numpy)\n",
    "    \n",
    "    # Get rid of Missing Data\n",
    "    dmatrix_df = pd.DataFrame(delta_matrix)   \n",
    "    dmatrix_fna = dmatrix_df.fillna(0)        \n",
    "    delta_matrix2 = dmatrix_fna.to_numpy()\n",
    "    \n",
    "    delta_betak = delta_matrix2.sum(axis=0, keepdims = True)       # summation in terms of examinees\n",
    "    \n",
    "    beta_med = beta_let - A * delta_betak                          # beta update by means of Gradient Descent\n",
    "    beta_result = beta_med - np.mean(beta_med)                     # standardization of beta\n",
    "\n",
    "    return beta_result                                            # The result is yielded in numpy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to update theta\n",
    "# Only the train_gagong_let is given in pandas.\n",
    "def set_theta(alpha_let, beta_let, theta_let, train_gagong_let):\n",
    "    \n",
    "    # Loading the common part\n",
    "    common_unit = preprocess_diff(alpha_let, beta_let, theta_let, train_gagong_let)    \n",
    "    \n",
    "    # Calculation Start\n",
    "    delta_matrix = alpha_let * common_unit            # before summation (formed in numpy)\n",
    "    \n",
    "    # Get rid of Missing Data\n",
    "    dmatrix_df = pd.DataFrame(delta_matrix)\n",
    "    dmatrix_fna = dmatrix_df.fillna(0)\n",
    "    delta_matrix2 = dmatrix_fna.to_numpy()\n",
    "    \n",
    "    delta_thetak = delta_matrix2.sum(axis=1, keepdims = True)   # summation in terms of items\n",
    "        \n",
    "    theta_result = theta_let - A * delta_thetak                 # theta update by means of Gradient Descent\n",
    "\n",
    "    return theta_result                                        # The result is yielded in numpy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to calculate KLD\n",
    "# Train_gagong_let is of pandas the others are of numpy.\n",
    "def set_D_KL(alpha_let, beta_let, theta_let, train_gagong_let):\n",
    "    \n",
    "    num_gagong = train_gagong_let.to_numpy()\n",
    "    P_imu = expect_model(alpha_let, beta_let, theta_let, train_gagong_let)\n",
    "    Q_imu = num_gagong.copy()\n",
    "    \n",
    "    KLD_imu_np = Q_imu * np.log((Q_imu) / (P_imu)) + (1 - Q_imu) * np.log((1 - Q_imu)/(1 - P_imu))\n",
    "    \n",
    "    # Get rid of missing data\n",
    "    KLD_imu_df = pd.DataFrame(KLD_imu_np)\n",
    "    KLD_shuttle = KLD_imu_df.fillna(0)\n",
    "    KLD_imu = KLD_shuttle.to_numpy()\n",
    "    \n",
    "    D_KL_mu = KLD_imu.sum(axis=1)\n",
    "    D_KL = D_KL_mu.sum(axis=0)\n",
    "    \n",
    "    return D_KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration function for model optimization\n",
    "\n",
    "def opt_model(alpha_let, beta_let, theta_let, train_gagong_let, test_gagong_let, num_iter):\n",
    "    \n",
    "    # Initialization of parameters and variables\n",
    "    alpha_test = alpha_let.copy()\n",
    "    beta_test = beta_let.copy()\n",
    "    theta_test = theta_let.copy()\n",
    "    \n",
    "    KLD_train = set_D_KL(alpha_test, beta_test, theta_test, train_gagong_let)\n",
    "    KLD_Trains = []\n",
    "    KLD_Trains.append(KLD_train)\n",
    "    \n",
    "    KLD_testset = set_D_KL(alpha_test, beta_test, theta_test, test_gagong_let)\n",
    "    KLD_Tests = []\n",
    "    KLD_Tests.append(KLD_testset)\n",
    "    \n",
    "    for k in tqdm(range(num_iter)):\n",
    "        # alpha update\n",
    "        alpha_carrier = set_alpha(alpha_test, beta_test, theta_test, train_gagong_let)\n",
    "        alpha_test = alpha_carrier\n",
    "\n",
    "        # beta update\n",
    "        beta_carrier = set_beta(alpha_test, beta_test, theta_test, train_gagong_let)\n",
    "        beta_test = beta_carrier\n",
    "\n",
    "        # theta update\n",
    "        theta_carrier = set_theta(alpha_test, beta_test, theta_test, train_gagong_let)\n",
    "        theta_test = theta_carrier\n",
    "\n",
    "        # calculation of Kullback-Leibler Divergence\n",
    "        KLD_carrier = set_D_KL(alpha_test, beta_test, theta_test, train_gagong_let)\n",
    "        KLD_testset = set_D_KL(alpha_test, beta_test, theta_test, test_gagong_let)\n",
    "        \n",
    "        # Determination whether the iteration keeps or not\n",
    "        if (k < num_iter - 1) and (KLD_carrier < KLD_train):\n",
    "            KLD_train = KLD_carrier\n",
    "            KLD_Trains.append(KLD_train)        # store KLD of trian set\n",
    "            KLD_Tests.append(KLD_testset)       # store KLD of test set\n",
    "        else:\n",
    "            print(\"Final Kullback-Leibler Divergence: \", KLD_train)\n",
    "            break\n",
    "    \n",
    "    return alpha_test, beta_test, theta_test, KLD_Trains, KLD_Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, it is very time to play the real game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "albetheKLD = []\n",
    "num_iter = 0\n",
    "\n",
    "for gagong_carrier in train_gagongs:\n",
    "    \n",
    "    carrier_shell = []\n",
    "    \n",
    "    p_df = gagong_carrier.copy()\n",
    "    num_np = p_df.to_numpy()\n",
    "\n",
    "    # theta initialization\n",
    "    row_pre = p_df.mean(axis=1)\n",
    "    row_prob_1 = row_pre.to_numpy()\n",
    "    row_prob = np.reshape(row_prob_1, (rows,1))\n",
    "\n",
    "    theta = np.log(row_prob/(1-row_prob))\n",
    "\n",
    "    # beta initialization\n",
    "    col_pre = p_df.mean(axis=0)\n",
    "    col_prob_1 = col_pre.to_numpy()\n",
    "    col_prob = np.array([col_prob_1])\n",
    "    beta0 = np.log(col_prob/(1-col_prob))\n",
    "    beta = np.mean(beta0) - beta0\n",
    "\n",
    "    # alpha initialization\n",
    "    alpha = np.ones((1,columns))\n",
    "\n",
    "    A = 0.002   # learning rate\n",
    "\n",
    "    alpha_mod, beta_mod, theta_mod, KLDs_mod, KLDs_test_mod = opt_model(alpha, beta, theta, p_df, test_gagongs[num_iter], 500)\n",
    "\n",
    "    carrier_shell.append(KLDs_mod)         # 0\n",
    "    carrier_shell.append(KLDs_test_mod)    # 1\n",
    "    carrier_shell.append(alpha_mod)        # 2\n",
    "    carrier_shell.append(beta_mod)         # 3\n",
    "    carrier_shell.append(theta_mod)        # 4\n",
    "\n",
    "    albetheKLD.append(carrier_shell)\n",
    "    num_iter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = []\n",
    "for vect in test_gagongs:\n",
    "    egg = vect.notnull().sum().sum()\n",
    "    num_tests.append(egg)\n",
    "    \n",
    "print(num_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_save = 0\n",
    "for carrying in albetheKLD:\n",
    "    num_save+=1\n",
    "    \n",
    "    KLD_trained = carrying[0]\n",
    "    KLD_tested = carrying[1]\n",
    "    \n",
    "    KLDs_train_pd = pd.DataFrame(KLD_trained)\n",
    "    KLDs_test_pd = pd.DataFrame(KLD_tested)\n",
    "\n",
    "    KLDs_train_pd.to_csv(\"UIRT{0}_KLDs_0.1train_0720.csv\".format(num_save))\n",
    "    KLDs_test_pd.to_csv(\"UIRT{0}_KLDs_0.1test_0720.csv\".format(num_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_save = 0\n",
    "for carrying in albetheKLD:\n",
    "    num_save+=1\n",
    "    \n",
    "    alpha_tested = carrying[2]\n",
    "    beta_tested = carrying[3]\n",
    "    theta_tested = carrying[4]\n",
    "    \n",
    "    alpha_tested_pd = pd.DataFrame(alpha_tested)\n",
    "    beta_tested_pd = pd.DataFrame(beta_tested)\n",
    "    theta_tested_pd = pd.DataFrame(theta_tested)\n",
    "    \n",
    "    alpha_tested_pd.to_csv(\"UIRT{0}_alpha_0.1_0701.csv\".format(num_save))\n",
    "    beta_tested_pd.to_csv(\"UIRT{0}_beta_0.1_0701.csv\".format(num_save))\n",
    "    theta_tested_pd.to_csv(\"UIRT{0}_theta_0.1_0701.csv\".format(num_save))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set vs Train set and Save the Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expect_simple_cal(alpha_let, beta_let, theta_let):\n",
    "    cal1 = np.exp(alpha_let * (theta_let - beta_let))/(1+np.exp(alpha_let * (theta_let - beta_let)))\n",
    "    \n",
    "    if cal1 >= 0.99:\n",
    "        cal1 = 0.99\n",
    "    elif cal1 <= 0.01:\n",
    "        cal1 = 0.01\n",
    "\n",
    "    cal_result = cal1                                    # 0 혹은 1 양극단 삭제\n",
    "\n",
    "    return cal_result                                           # 숫자로 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_final = []\n",
    "num_unit = 0\n",
    "#test_trials = []\n",
    "#test_trials.append(test_gagongs[0])\n",
    "for gagong_unit in test_gagongs:\n",
    "#for gagong_unit in test_trials:\n",
    "    # pick up a basket of parameters\n",
    "    basket_picks = albetheKLD.copy()[num_unit]\n",
    "\n",
    "    # update the index\n",
    "    num_unit += 1\n",
    "        \n",
    "    # theta에 index 묻히기\n",
    "    theta_fin_df = pd.DataFrame(basket_picks[4])\n",
    "        \n",
    "    # alpha와 beta에 column index 묻히기\n",
    "    alpha_fin_df = pd.DataFrame(basket_picks[2])\n",
    "    beta_fin_df = pd.DataFrame(basket_picks[3])\n",
    "\n",
    "    alpha_fin_df.columns = fil4.columns.to_list()\n",
    "    beta_fin_df.columns = fil4.columns.to_list()\n",
    "    \n",
    "    # theta, alpha, beta 모두를 끼워넣기\n",
    "    threshed_theta = []\n",
    "\n",
    "    # Set the coordinate\n",
    "    coord_pd = pd.read_csv(\"UIRT_0.1testset_{0}_0701.csv\".format(num_unit))\n",
    "    coord_pd.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    sampl_len = coord_pd.shape[1]\n",
    "    \n",
    "    for th in range(sampl_len):\n",
    "        piece_th = []\n",
    "        \n",
    "        coord_x = int(coord_pd.loc[0][th])\n",
    "        coord_col = coord_pd.loc[1][th]\n",
    "        correctness = float(coord_pd.loc[2][th])\n",
    "\n",
    "        theta_piece = theta_fin_df.loc[coord_x][0]\n",
    "        alpha_piece = alpha_fin_df.loc[0][coord_col]\n",
    "        beta_piece = beta_fin_df.loc[0][coord_col]\n",
    "        expect_cal = expect_simple_cal(alpha_piece, beta_piece, theta_piece)\n",
    "\n",
    "        piece_th.append(coord_x)         # 0\n",
    "        piece_th.append(coord_col)       # 1\n",
    "        piece_th.append(correctness)     # 2\n",
    "        piece_th.append(theta_piece)     # 3\n",
    "        piece_th.append(alpha_piece)     # 4\n",
    "        piece_th.append(beta_piece)      # 5\n",
    "        piece_th.append(expect_cal)      # 6\n",
    "        threshed_theta.append(piece_th)  # th\n",
    "\n",
    "    #print(threshed_theta)       # 순서: 학생 index, 문제 index, 문제 정오, \n",
    "                                        # theta, alpha, beta, 모델계산값\n",
    "    \n",
    "    threshed_pick = threshed_theta.copy()\n",
    "    \n",
    "    # 정오답 확실히 판정했는지 여부 판단\n",
    "    stud_info_simple = []\n",
    "    num_R = 0\n",
    "    num_W = 0\n",
    "    num_tot = 0\n",
    "\n",
    "    for student in threshed_pick:\n",
    "        carrier_bot = []\n",
    "        data_real = student[2]\n",
    "        data_cal = student[6]\n",
    "        data_jud = 0\n",
    "        data_RW = ''\n",
    "\n",
    "#        if data_cal >=0.7:                 # rounding off to the nearest integer\n",
    "#            data_jud = 0.99\n",
    "        if data_cal >=0.5:                 # rounding off to the nearest integer\n",
    "            data_jud = 0.99\n",
    "#        elif data_cal <= 0.3:\n",
    "#            data_jud = 0.01\n",
    "#        else:\n",
    "#            data_jud = 0.5\n",
    "        else:\n",
    "            data_jud = 0.01\n",
    "\n",
    "        if data_real == data_jud:\n",
    "            data_RW = 'O'                   # 'O'는 model이 실제 정오답 여부를 올바르게 판정했다는 의미\n",
    "            num_R += 1\n",
    "            num_tot += 1\n",
    "        else:\n",
    "            data_RW = 'X'                   # 'X'는 model이 실제 정오답 여부를 올바르게 판정하지 못했다는 의미\n",
    "            num_W += 1\n",
    "            num_tot +=1\n",
    "\n",
    "        carrier_bot.append(student[0])\n",
    "        carrier_bot.append(student[1])\n",
    "        carrier_bot.append(data_RW)\n",
    "        stud_info_simple.append(carrier_bot)\n",
    "\n",
    "    #print(stud_info_simple)\n",
    "    print(\"{0}번째 판정 성공률: {1}\".format(num_unit, num_R/num_tot * 100))\n",
    "    \n",
    "    # 판정 결과 데이터로 저장\n",
    "    threshed_pick_df = pd.DataFrame(threshed_pick)\n",
    "    threshed_pick_df.to_csv(\"Judgement_mid_UIRT{0}_0701.csv\".format(num_unit))\n",
    "    \n",
    "    # 보기 좋게 다시 정리\n",
    "    stud_info_np = np.array(stud_info_simple)\n",
    "    stud_info_T = np.transpose(stud_info_np)\n",
    "    #print(stud_info_T)\n",
    "    stud_info_df = pd.DataFrame(stud_info_T)\n",
    "    stud_info_df.rename(index={0: \"Stud #\", 1: \"Prob #\", 2: \"Judge\"}, inplace=True)\n",
    "    #print(stud_info_df)\n",
    "    \n",
    "    # 보여주기 및 저장\n",
    "    basket_final.append(stud_info_df)\n",
    "    stud_info_df.to_csv(\"Judgement_fin_UIRT{0}_0701.csv\".format(num_unit))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
